{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcbNBw9PL0OexX7mkRCh5e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrishaameenaa-cmd/Data_Processing_Challenge/blob/main/23BCS160_Real_Time_Data_Streaming_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi9xTy62-MET",
        "outputId": "85bde84f-e1de-4ee7-b1d8-d0ba9dbc7327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Downloading kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.2.15\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pyspark==3.5.1 kafka-python\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "import random\n"
      ],
      "metadata": {
        "id": "tYU-UnSnJpRG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RealTimeDataSimulation\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "-07KOQ6JJzeN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round, avg\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, 30.56, 60.2),\n",
        "    (2, 27.34, 55.7),\n",
        "    (3, 32.15, 58.4)\n",
        "]\n",
        "columns = [\"sensor_id\", \"temperature\", \"humidity\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# ❌ Wrong: round() from Python\n",
        "# df = df.withColumn(\"rounded_temp\", round(df[\"temperature\"], 2))\n",
        "\n",
        "# ✅ Correct: round() from pyspark.sql.functions\n",
        "df = df.withColumn(\"rounded_temp\", round(col(\"temperature\"), 2))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xm-DYhKJ4Ys",
        "outputId": "64b396da-9567-4fcf-c388-c80fd29a1f5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+--------+------------+\n",
            "|sensor_id|temperature|humidity|rounded_temp|\n",
            "+---------+-----------+--------+------------+\n",
            "|        1|      30.56|    60.2|       30.56|\n",
            "|        2|      27.34|    55.7|       27.34|\n",
            "|        3|      32.15|    58.4|       32.15|\n",
            "+---------+-----------+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcndDXF1KMbq",
        "outputId": "c29d584a-5dd1-4394-8a72-a8e948f35f02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round, avg, lit\n",
        "import pandas as pd\n",
        "import random, time\n"
      ],
      "metadata": {
        "id": "wAZWZFEsLAST"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"RealTimeStreamingSim\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "flZ19LUuLHfl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round as spark_round, avg\n"
      ],
      "metadata": {
        "id": "en2esUt9Lfrz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round as spark_round, avg\n",
        "import pandas as pd, random, time\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RealTimeStreamingSim\").getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "VA5yO7LaLwhl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sensor_data():\n",
        "    df = pd.DataFrame({\n",
        "        \"sensor_id\": [random.randint(1,5) for _ in range(5)],\n",
        "        \"temperature\": [round(random.uniform(25,35),2) for _ in range(5)],\n",
        "        \"humidity\": [round(random.uniform(40,70),2) for _ in range(5)],\n",
        "        \"timestamp\": [int(time.time()) for _ in range(5)]\n",
        "    })\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Oc4UMmXKL7g8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round as spark_round, avg\n",
        "import pandas as pd\n",
        "import random, time\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RealTimeStreamingSim\").getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "KZMbaBAGL-bd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sensor_data():\n",
        "    data = {\n",
        "        \"sensor_id\": [random.randint(1,5) for _ in range(5)],\n",
        "        \"temperature\": [round(random.uniform(25,35),2) for _ in range(5)],\n",
        "        \"humidity\": [round(random.uniform(40,70),2) for _ in range(5)],\n",
        "        \"timestamp\": [int(time.time()) for _ in range(5)]\n",
        "    }\n",
        "    return pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "roEmHnBUMHBa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round as spark_round, avg\n",
        "import pandas as pd\n",
        "import random, time\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RealTimeStreamingSim\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "5xH6ssROMKOZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_sensor_data():\n",
        "    data = {\n",
        "        \"sensor_id\": [random.randint(1,5) for _ in range(5)],\n",
        "        \"temperature\": [round(random.uniform(25,35),2) for _ in range(5)],\n",
        "        \"humidity\": [round(random.uniform(40,70),2) for _ in range(5)],\n",
        "        \"timestamp\": [int(time.time()) for _ in range(5)]\n",
        "    }\n",
        "    return pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "Qk1DmHwpMfj7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Simulate streaming batches\n",
        "for batch in range(3):\n",
        "    print(f\"\\n=== Batch {batch+1} ===\")\n",
        "\n",
        "    pandas_df = generate_sensor_data()              # ✅ Just Pandas\n",
        "    spark_df = spark.createDataFrame(pandas_df)     # ✅ Now to Spark\n",
        "\n",
        "    # Processing safely inside Spark\n",
        "    processed_df = (\n",
        "        spark_df\n",
        "        .withColumn(\"rounded_temp\", spark_round(col(\"temperature\"), 2))\n",
        "        .groupBy(\"sensor_id\")\n",
        "        .agg(\n",
        "            avg(\"temperature\").alias(\"avg_temp\"),\n",
        "            avg(\"humidity\").alias(\"avg_humidity\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    processed_df.show()\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdpVw59BMij7",
        "outputId": "5d971c35-bbe3-430d-aab2-00a4f0dc8330"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Batch 1 ===\n",
            "+---------+------------------+------------+\n",
            "|sensor_id|          avg_temp|avg_humidity|\n",
            "+---------+------------------+------------+\n",
            "|        1|28.134999999999998|      57.875|\n",
            "|        4|             27.77|       68.51|\n",
            "|        5|             28.82|       49.79|\n",
            "|        2|             27.12|       55.51|\n",
            "+---------+------------------+------------+\n",
            "\n",
            "\n",
            "=== Batch 2 ===\n",
            "+---------+--------+------------+\n",
            "|sensor_id|avg_temp|avg_humidity|\n",
            "+---------+--------+------------+\n",
            "|        3|   28.56|       54.37|\n",
            "|        2|  28.695|       54.53|\n",
            "|        5|   29.74|       66.95|\n",
            "|        1|   29.07|       56.95|\n",
            "+---------+--------+------------+\n",
            "\n",
            "\n",
            "=== Batch 3 ===\n",
            "+---------+--------+------------+\n",
            "|sensor_id|avg_temp|avg_humidity|\n",
            "+---------+--------+------------+\n",
            "|        5|   27.32|       61.53|\n",
            "|        2|   31.86|      49.785|\n",
            "|        4|  30.245|       58.25|\n",
            "+---------+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import ML dependencies\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Create dummy training data (simulate historical sensor data)\n",
        "train_X = np.random.uniform(low=25, high=35, size=(100, 2))  # temperature, humidity\n",
        "train_y = (train_X[:,0] > 30).astype(int)  # label 1 if temp > 30 else 0 (high temp)\n",
        "\n",
        "# Step 3: Scale and train the model\n",
        "scaler = StandardScaler()\n",
        "train_X_scaled = scaler.fit_transform(train_X)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_X_scaled, train_y)\n",
        "\n",
        "print(\"✅ Model trained for predicting 'High Temperature Risk'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWnoq1jCMpdU",
        "outputId": "a6723a3f-6c5a-4033-d2e2-5c9f9b2f7e43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model trained for predicting 'High Temperature Risk'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Create a training dataset (dummy for demonstration)\n",
        "train_data = [\n",
        "    (1, 28.5, 50.0, 0),  # normal\n",
        "    (2, 30.2, 60.0, 0),  # normal\n",
        "    (3, 33.1, 45.0, 1),  # high\n",
        "    (4, 35.0, 65.0, 1),  # high\n",
        "]\n",
        "columns = [\"sensor_id\", \"temperature\", \"humidity\", \"label\"]\n",
        "train_df = spark.createDataFrame(train_data, columns)\n",
        "\n",
        "# Features and model\n",
        "assembler = VectorAssembler(inputCols=[\"temperature\", \"humidity\"], outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "print(\"✅ Model trained successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFd3Z1b8MyMF",
        "outputId": "db1de873-8fb9-4650-eeda-91089f87ee84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round as spark_round, avg\n",
        "import pandas as pd\n",
        "import random, time\n",
        "\n",
        "def generate_sensor_data():\n",
        "    return pd.DataFrame({\n",
        "        \"sensor_id\": [random.randint(1,5) for _ in range(5)],\n",
        "        \"temperature\": [round(random.uniform(25,35),2) for _ in range(5)],\n",
        "        \"humidity\": [round(random.uniform(40,70),2) for _ in range(5)],\n",
        "        \"timestamp\": [int(time.time()) for _ in range(5)]\n",
        "    })\n",
        "\n",
        "for batch in range(3):\n",
        "    print(f\"\\n=== Batch {batch+1} ===\")\n",
        "\n",
        "    pandas_df = generate_sensor_data()\n",
        "    spark_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "    # Apply trained model\n",
        "    predictions = model.transform(spark_df)\n",
        "    predictions.select(\"sensor_id\", \"temperature\", \"humidity\", \"prediction\").show()\n",
        "\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1O5kYG7QbP0",
        "outputId": "0d3e50af-7155-4c40-d8f9-460cb3edb949"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Batch 1 ===\n",
            "+---------+-----------+--------+----------+\n",
            "|sensor_id|temperature|humidity|prediction|\n",
            "+---------+-----------+--------+----------+\n",
            "|        4|      32.43|   57.49|       1.0|\n",
            "|        3|      31.09|   59.24|       0.0|\n",
            "|        2|      27.12|   59.86|       0.0|\n",
            "|        3|       27.4|   54.86|       0.0|\n",
            "|        5|      30.15|   62.79|       0.0|\n",
            "+---------+-----------+--------+----------+\n",
            "\n",
            "\n",
            "=== Batch 2 ===\n",
            "+---------+-----------+--------+----------+\n",
            "|sensor_id|temperature|humidity|prediction|\n",
            "+---------+-----------+--------+----------+\n",
            "|        5|      33.36|   47.54|       1.0|\n",
            "|        2|      33.77|   49.33|       1.0|\n",
            "|        4|      28.88|   57.38|       0.0|\n",
            "|        3|      33.62|   59.04|       1.0|\n",
            "|        5|      26.24|    54.1|       0.0|\n",
            "+---------+-----------+--------+----------+\n",
            "\n",
            "\n",
            "=== Batch 3 ===\n",
            "+---------+-----------+--------+----------+\n",
            "|sensor_id|temperature|humidity|prediction|\n",
            "+---------+-----------+--------+----------+\n",
            "|        1|      26.67|   54.64|       0.0|\n",
            "|        5|      25.48|   55.06|       0.0|\n",
            "|        4|      25.16|   49.86|       0.0|\n",
            "|        2|       31.1|   46.97|       1.0|\n",
            "|        3|      31.12|   52.35|       0.0|\n",
            "+---------+-----------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5_FgRTkQgSK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
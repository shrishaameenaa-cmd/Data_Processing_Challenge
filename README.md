# 🧩 Data_Processing_Challenge  
### “Data Preprocessing Challenge (PySpark)”

This project demonstrates data cleaning and preprocessing using **Apache Spark (PySpark)** on large-scale datasets.

---

## 🚀 Quick Launch
Open the notebook directly in **Google Colab**:  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shrishaameenaa-cmd/Data_Processing_Challenge/blob/main/23BCS160_Data_Preprocessing_Challenge.ipynb)

---

## 🧠 Project Overview
The goal of this challenge is to preprocess raw data efficiently using **PySpark**, ensuring it is clean, consistent, and ready for analytics or machine-learning workflows.

### ✅ Key Tasks
- Handle missing values  
- Fix data-type inconsistencies  
- Remove duplicate records  
- Normalize / standardize data  
- Perform basic feature engineering  
- Validate data integrity after processing  

---

## ⚙️ Tech Stack
- **Apache Spark (PySpark)**  
- **Google Colab / Jupyter Notebook**  
- **Python 3**  
- **Pandas & NumPy** (for additional transformations)

---

## 🏃‍♀️ How to Run
1. Open the notebook in **Google Colab** (click the button above).  
2. Upload your dataset to the Colab environment or connect to Google Drive.  
3. Run the cells sequentially to preprocess the data.  
4. Review the cleaned output and summary statistics.

---

## 📂 Repository Structure
Data_Processing_Challenge/
│
├── 23BCS160_Data_Preprocessing_Challenge.ipynb # Main PySpark Notebook
└── README.md # Project documentation

---

## 📜 Author
**Shrishaa Meenaa**  
_Data Processing Challenge — 2025_
